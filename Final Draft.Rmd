---
title: "Final Draft"
author: "Zhenghao Lin, Chenjia Liu, Cynthia Zhou"
date: "2024-04-23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction, Motivations, Relevance, Objectives
Climate change has become one of the most pressing issues facing humanity in the 21st century. With the escalating impacts of global warming, extreme weather events, and environmental degradation becoming more visible, public awareness and concern have surged. In this digital age, the internet has become a primary avenue for education and engagement on environmental issues. As the world's predominant search engine, Google offers invaluable insights into global interest in climate topics, making it a valuable resource for public concern evaluation.

This report utilizes Google Trends data to perform a comprehensive time series analysis of the search volume for "Climate" to identify underlying patterns, trends, and anomalies in public interest over time. Using advanced statistical tools and methods, this analysis will:
1. Forecast Future Trends: Apply various models to the original dataset to predict future changes in public interest concerning climate issues.
2. Assess the Impact of Outliers: Compare forecasts from the original data with those from a modified dataset that excludes outliers, evaluating how these anomalies affect overall predictions.
3. Analyze Spatial Variations: Conduct a spatial analysis of relative search volumes to map regional differences in interest regarding climate-related topics across the globe.

Through these analyses, the report seeks to provide a comprehensive overview of the public's engagement with climate issues, offering insights that could inform policymakers, educators, and environmental organizations in their efforts to foster greater awareness and action.

# Data Information
Our data set is sourced from Google Trends. As the original data is interest index which only represents the popularity of total search term in certain time periods or regions, we use a Chrome extension called Glimpse-Google Trend Supercharged to convert interest index into absolute values. We captured the number of searches for climate-related search terms globally from 2004 to the present.

```{r data, echo=FALSE}
datainfor <- data.frame( Data = c("Source: ", "Search Term: ", "Time Range: ", "Spatial Range: "), 
                         Information = c("Google Trend", "Climate","2004 to present","Global")
                        )
knitr::kable(datainfor, caption = "Data Information")
```

```{r include=FALSE}
library(ggplot2)
library(forecast)  
library(Kendall)
library(tseries)
library(outliers)
library(tidyverse)
library(smooth)
library(zoo)
library(kableExtra)
library(cowplot)
library(GGally)
library(readxl)
library(wbstats) 
library(purrr)
library(sf)
library('rnaturalearth')
library(sp)
```
#  Methodology/Analysis

```{r include=FALSE}
raw_df<-read.csv("./Data/climate-timeline_Glimpse_Google-Trends.csv", skip = 5) 

df <- raw_df %>%
  mutate(Time = as.Date(Time, format="%Y-%m-%d")) %>% 
  rename(Date = Time,
         Normalized_Value = `Normalized.Value..0.100.`,
         Absolute_Volume = `Absolute.Google.Search.Volume`)
```

```{r echo=FALSE}
ts_df <- msts(df$Absolute_Volume, 
                           seasonal.periods =c(3,12),
                           start=c(2003,12))

ts_df_training <- subset(ts_df,end = length(ts_df)-12)
                         
ts_df_testing <- subset(ts_df,start = length(ts_df)-12)

ts_df_training %>% mstl() %>%
  autoplot()
```
First, the data set was read and the Time is converted to "date". The variable of interest (absolute search volume) was renamed and converted to a time series object. A training set and test set was then obtained by filtering data. The last 12 observation was selected as holdouts. The time series data set was plotted to check for trends and seasonality. The seasonal period is set using 3 (quarterly) and 12 (manually).

```{r echo=FALSE, warning=FALSE}
plot_grid(
  autoplot(Acf(df$Absolute_Volume, lag = 40, plot=FALSE), 
                main = "ACF Absolute Search Volume"),
  autoplot(Pacf(df$Absolute_Volume, lag = 40, plot=FALSE),  
                  main = "PACF Absolute Search Volume")
)

```
The ACF plot displays the correlation between the time series data and its lagged values over different time lags. The plot shows several spikes that are outside the blue dashed confidence interval lines, especially at the initial lags. This suggests that there is a significant autocorrelation at those lags. The PACF plot, on the other hand, shows the partial correlation between the time series and its lagged values, controlling for the values at shorter lags. The significant spike at the first lag indicates a strong correlation. Since the spike at lag 1 is followed by a cutoff, it might suggest an AR(1) component.The slow decay in the ACF plot could indicate that the data is non-stationary or has seasonal patterns.

The models that are being used by this analysis include TBATS (Trigonometric seasonality, Box-Cox transformation, ARMA errors, Trend and
 Seasonal components) model, ARIMA + FOURIER terms, STL+ETS (Seasonal and Trend decomposition using Loess + Error, Trend, Seasonal) model, Neural Network, and SSES (State Space Exponential Smoothing) model. A short description of the models are given below:
 
## Description
### 1 TBATS
TBATS was designed to forecast time series with multiple seasonal periods. For example, daily data may have a weekly pattern as well as an annual pattern. Or hourly data can have three seasonal periods: a daily pattern, a weekly pattern, and an annual pattern. In TBATS, a Box-Cox transformation is applied to the original time series, and then this is modelled as a linear combination of an exponentially smoothed trend, a seasonal component and an ARMA component. The seasonal components are modelled by trigonometric functions via Fourier series. TBATS conducts some hyper-parameter tuning (e.g. which of these components to keep and which to discard) using AIC.

### 2 ARIMA + FOURIER terms
The ARIMA model with Fourier terms for seasonal adjustment is a sophisticated approach for forecasting time series data, particularly when the data exhibits complex seasonality that cannot be adequately modeled by simple seasonal ARIMA (SARIMA) components alone.

### 3 STL+ETS
STL is a versatile and robust method for decomposing time series. STL is an acronym for “Seasonal and Trend decomposition using Loess”, while loess is a method for estimating nonlinear relationships. The ETS models are a family of time series models with an underlying state space model consisting of a level component, a trend component (T), a seasonal component (S), and an error term (E).

### 4 Neural Network
Neural networks or simulated neural networks are a subset of machine learning which is inspired by the human brain. They mimic how biological neurons communicate with one another to come up with a decision. A neural network consists of an input layer, a hidden layer, and an output layer. The first layer receives raw input, it is processed by multiple hidden layers, and the last layer produces the result. 

### 5 SSES
The exponential smoothing state space model is a statistical model that generates prediction intervals and point forecasts. It is a stochastic data generating process that can produce an entire forecast distribution. The exponential smoothing methods generate point forecasts, and the statistical models generate the same point forecasts, but can also generate prediction intervals.

```{r echo=FALSE}
#TBATS
TBATS_fit <- tbats(ts_df_training)

TBATS_for <- forecast(TBATS_fit, h=12)

autoplot(ts_df_testing) +
  autolayer(TBATS_for, series="TBATS",PI=FALSE)

TBATS_scores <- accuracy(TBATS_for$mean,ts_df_testing)
```

```{r echo=FALSE}
#ARIMA
ARIMA_fit <- auto.arima(ts_df_training, 
                             seasonal=FALSE, 
                             lambda=0,
                             xreg=fourier(ts_df_training, 
                                          K=c(1,2))
                             )


ARIMA_for <- forecast(ARIMA_fit,
                           xreg=fourier(ts_df_training,
                                        K=c(1,2),
                                        h=12),
                           h=12
                           )

autoplot(ARIMA_for) + ylab("Search Volume")

autoplot(ts_df_testing) +
  autolayer(ARIMA_for, series="ARIMA_FOURIER",PI=FALSE) +
  ylab("Search Volume")

ARIMA_scores <- accuracy(ARIMA_for$mean,ts_df_testing)
```

```{r echo=FALSE}
#STL+ETS
ETS_fit <-  stlf(ts_df_training,h=12)
autoplot(ETS_fit) + ylab("Search Volume")
autoplot(ts_df_testing) +
  autolayer(ETS_fit, series="STL + ETS",PI=FALSE) +
  ylab("Search Volume")

ETS_scores <- accuracy(ETS_fit$mean,ts_df_testing)
```

```{r echo=FALSE}
#NN
NN_fit <- nnetar(ts_df_training,p=0,P=1,xreg=fourier(ts_df_training, K=c(1,4)))

NN_for <- forecast(NN_fit, h=12,xreg=fourier(ts_df_training, 
                                          K=c(1,4),h=12))

#Plot model + observed data
autoplot(ts_df_testing) +
  autolayer(NN_for, series="Neural Network",PI=FALSE)+
  ylab("Search Volume") 

NN_scores <- accuracy(NN_for$mean,ts_df_testing)
```

```{r echo=FALSE}
#SSES
SSES_seas <- es(ts_df_training,model="AMM",h=12,holdout=FALSE)

autoplot(ts_df_testing) +
  autolayer(SSES_seas$forecast, series="SSES")+
  ylab("Search Volume") 

SSES_scores <- accuracy(SSES_seas$forecast,ts_df_testing)
```

```{r include=FALSE}
# SCORE
scores <- as.data.frame(
  rbind(ETS_scores, ARIMA_scores, TBATS_scores, NN_scores, SSES_scores)
  )
row.names(scores) <- c("STL+ETS", "ARIMA+Fourier","TBATS","NN","SSES")

#choose model with lowest RMSE
best_model_index <- which.min(scores[,"RMSE"])
cat("The best model by RMSE is:", row.names(scores[best_model_index,]))
```

```{r echo=FALSE}
#SCORE TABLE
kbl(scores, 
      caption = "Forecast Accuracy for Daily Active Power",
      digits = array(5,ncol(scores))) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = "hold_position") %>%
  #highlight model with lowest RMSE
  kable_styling(latex_options="striped", stripe_index = which.min(scores[,"RMSE"]))
```
The data was trained from December 2003 to March 2023 and tested from April 2023 to March 2024. Predictions were made for the next 12 months. Based on the residual scores, the neural network (NN) model outperformed others, achieving the lowest RMSE of 353,492.8. In contrast, the STL+ETS model registered the highest RMSE at 1,339,766.2. Consequently, the NN model was selected for predicting the absolute search volume. It's important to note that the residual scores were quite high. This high error rate is partly because our base numbers are substantial, typically in the millions. The NN's residual score, although in the hundreds of thousands, represents about 10 percent of the data, which is still considered a significant error. Moving forward, the next step involves removing outliers, which are causing huge spikes, and rerunning the candidate model.

```{r echo=FALSE}
#FORECAST
NN_fit_2 <- nnetar(ts_df,p=0,P=1,xreg=fourier(ts_df, K=c(1,4)))

NN_for_2 <- forecast(NN_fit_2, h=12,xreg=fourier(ts_df, 
                                          K=c(1,4),h=12))

#Plot FORECAST
autoplot(NN_for_2)+
  ylab("Search Volume") 


```

# Outlier Removal and Model Optimization
This section of the report builds upon the initial time series analysis by addressing outliers in the dataset and re-evaluating the forecasting models to identify the best performer in a refined dataset. The focus is to enhance model accuracy by minimizing the distortion effects caused by outliers.

## Outlier Detection and Interpolation
The process began by identifying outliers using the Interquartile Range (IQR) method, a robust technique that determines extreme values based on the distribution's quartiles. Outliers were defined as observations that fall below the first quartile minus 1.5 times the IQR or above the third quartile plus 1.5 times the IQR. These outliers were then replaced using spline interpolation, which provides a smooth estimate that is less likely to be influenced by extreme values, hence maintaining the integrity of the time series.

## Methodology
The outliers in the Absolute_Volume data were first visually identified and marked in the dataset. Below is the graph of the time series plot with outliers identified through IQR method.

```{r echo=FALSE, warning=FALSE}
#Identify the outliers
IQR_value <- IQR(df$Absolute_Volume, na.rm = TRUE)
upper_bound <- quantile(df$Absolute_Volume, 0.75, na.rm = TRUE) + 1.5 * IQR_value
lower_bound <- quantile(df$Absolute_Volume, 0.25, na.rm = TRUE) - 1.5 * IQR_value

# Add a new column for outliers
df$Outlier <- ifelse(df$Absolute_Volume > upper_bound | df$Absolute_Volume < lower_bound, "Yes", "No")

ggplot(df, aes(x = Date, y = Absolute_Volume)) + 
  geom_line(color = "blue", size = 1) + 
  geom_point(aes(color = Outlier), size = 2) +  # Points colored based on outlier status
  scale_color_manual(values = c('No' = "blue", 'Yes' = "red")) +  # Red for outliers
  labs(title = "Time Series Residuals with Outliers Highlighted",
       x = "Date",
       y = "Absolute_Volume")
```

The na.spline function from the zoo package was then used to interpolate and fill in these outliers based on neighboring values, which helps in preserving the trend and seasonal patterns of the series. The following figure from the analysis illustrates the original data against the interpolated data, showing how the outliers have been integrated.
```{r echo=FALSE}
df$Interpolated_Volume <- df$Absolute_Volume

outliers_indices <- which(df$Outlier == "Yes")
df$Interpolated_Volume[outliers_indices] <- NA

df$Interpolated_Volume <- na.spline(df$Interpolated_Volume)

ggplot(df, aes(x = Date)) + 
  geom_line(aes(y = Absolute_Volume, colour = "Original"), size=1, alpha=0.5) +
  geom_line(aes(y = Interpolated_Volume, colour = "Interpolated"), size=1) +
  scale_colour_manual("", 
                      breaks = c("Original", "Interpolated"),
                      values = c("grey", "blue")) +
  labs(title = "Original vs Interpolated Time Series",
       x = "Date",
       y = "Volume")

```
## Model Re-evaluation
With the outliers addressed, the dataset was split into training and testing sets. The analysis method and forecasting models used in the previous section were re-applied to this cleaned data.
```{r echo=FALSE}
clean_df <- df %>%
  select(Date, Interpolated_Volume)

ts_clean_df <- msts(df$Interpolated_Volume, 
                           seasonal.periods =c(3,12),
                           start=c(2003,12))

ts_clean_df_training <- subset(ts_clean_df,end = length(ts_clean_df)-12)
                         
ts_clean_df_testing <- subset(ts_clean_df,start = length(ts_clean_df)-12)

ts_clean_df_training %>% mstl() %>%
  autoplot()
```
When looking at the ACF and PACF plots of the cleaned data frame, the ACF plot reveals strong positive autocorrelations at the initial lags and then gradually decline as the lags increase, indicating potential non-stationarity within the time series data. Furthermore, the ACF plot exhibits a potential seasonal pattern through its recurring spikes at specific lags.
For the PACF plot, since it has a significant spike at the first lag and then cut off after it, the plot suggests at least an AR(1) model. This result align with the observation with the ACF and PACF plots of the original dataset.

```{r echo=FALSE, warning=FALSE}
plot_grid(
  autoplot(Acf(clean_df$Interpolated_Volume, lag = 40, plot=FALSE), 
                main = "ACF Absolute Search Volume"),
  autoplot(Pacf(clean_df$Interpolated_Volume, lag = 40, plot=FALSE),  
                  main = "PACF Absolute Search Volume")
)
```
```{r echo=FALSE}
#TBATS
TBATS_fit_clean <- tbats(ts_clean_df_training)

TBATS_for_clean <- forecast(TBATS_fit_clean, h=12)

autoplot(ts_clean_df_testing) +
  autolayer(TBATS_for_clean, series="TBATS",PI=FALSE)

TBATS_scores_clean <- accuracy(TBATS_for_clean$mean,ts_clean_df_testing)
```

```{r echo=FALSE}
#ARIMA
ARIMA_fit_clean <- auto.arima(ts_clean_df_training, 
                             seasonal=FALSE, 
                             lambda=0,
                             xreg=fourier(ts_clean_df_training, 
                                          K=c(1,2))
                             )


ARIMA_for_clean <- forecast(ARIMA_fit_clean,
                           xreg=fourier(ts_clean_df_training,
                                        K=c(1,2),
                                        h=12),
                           h=12
                           )

autoplot(ARIMA_for_clean) + ylab("Search Volume")

autoplot(ts_clean_df_testing) +
  autolayer(ARIMA_for_clean, series="ARIMA_FOURIER",PI=FALSE) +
  ylab("Search Volume")

ARIMA_scores_clean <- accuracy(ARIMA_for_clean$mean,ts_clean_df_testing)
```
```{r echo=FALSE}
#STL+ETS
ETS_fit_clean <-  stlf(ts_clean_df_training,h=12)
autoplot(ETS_fit_clean) + ylab("Search Volume")
autoplot(ts_clean_df_testing) +
  autolayer(ETS_fit_clean, series="STL + ETS",PI=FALSE) +
  ylab("Search Volume")

ETS_scores_clean <- accuracy(ETS_fit_clean$mean,ts_clean_df_testing)
```
```{r echo=FALSE}
#NN
NN_fit_clean <- nnetar(ts_clean_df_training,p=0,P=1,xreg=fourier(ts_clean_df_training, K=c(1,4)))

NN_for_clean <- forecast(NN_fit_clean, h=12,xreg=fourier(ts_clean_df_training, 
                                          K=c(1,4),h=12))

#Plot model + observed data
autoplot(ts_clean_df_testing) +
  autolayer(NN_for_clean, series="Neural Network",PI=FALSE)+
  ylab("Search Volume") 

NN_scores_clean <- accuracy(NN_for_clean$mean,ts_clean_df_testing)
```
```{r echo=FALSE}
#SSES
SSES_seas_clean <- es(ts_clean_df_training,model="AMM",h=12,holdout=FALSE)

autoplot(ts_clean_df_testing) +
  autolayer(SSES_seas_clean$forecast, series="SSES")+
  ylab("Search Volume") 

SSES_scores_clean <- accuracy(SSES_seas_clean$forecast,ts_clean_df_testing)

```

```{r include=FALSE}
# SCORE for clean_df
scores_clean <- as.data.frame(
  rbind(ETS_scores_clean, ARIMA_scores_clean, TBATS_scores_clean, NN_scores_clean, SSES_scores_clean)
  )
row.names(scores_clean) <- c("STL+ETS", "ARIMA+Fourier","TBATS","NN","SSES")
```
## Results
Below is the model score table of cleaned dataset with outliers removed. Compared to the original data frame, the cleaning process significantly improved model performance. The TBATS model emerged as the best model with the lowest RMSE, indicating the highest forecasting accuracy among the evaluated models on the cleaned dataset. The improvement underscores the impact of outliers removal on enhancing model reliability and accuracy.

```{r echo=FALSE}
#SCORE TABLE
kbl(scores_clean, 
      caption = "Forecast Accuracy for Daily Active Power with Cleaned Dataset",
      digits = array(5,ncol(scores_clean))) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = "hold_position") %>%
  #highlight model with lowest RMSE
  kable_styling(latex_options="striped", stripe_index = which.min(scores_clean[,"RMSE"]))
```
Below is the forecast of the search volume of term "Climate" on google for next 12 months with TBATS forecasting model using cleaned dataset with outliers removded.
```{r echo=FALSE}
#FORECAST for clean_df
TBATS_fit_clean_2 <- tbats(ts_clean_df)

TBATS_for_clean_2 <- forecast(TBATS_fit_clean_2, h=12)

autoplot(TBATS_for_clean_2) +
  ylab("Search Volume") 

```
## Conclusion
The second part of the analysis highlighted the critical role of preprocessing steps, like outliers removal, in predictive analytics. By cleaning the data, all models performed better, with TBATS being the most effective in forecasting under the new dataset conditions. This phase not only provided insights into more reliable forecasting but also demonstrated the necessity of robust data handling methods in analytics workflows.

## Limitations
One limitation in this section is the potential risk of overfitting our models. While we have implemented sophisticated techniques to interpolate and remove outliers, there is an inherent danger that our models may become too finely tuned to our existing data. This overfitting can easily lead to less accurate forecasts.
Moreover, our current dataset may not be extensive enough to conclusively differentiate between genuine seasonal effects and anomalies. Some of the spikes might not represent underlying seasonal trends but could instead be one-time events or products of irregular fluctuations. These non-recurring spikes could potentially skew the forecasts.
To mitigate these concerns and refine our understanding of the time series, it is recommended that we continue to collect data for several more years. An expanded dataset over a longer time horizon will enable us to more reliably conclusion whether the observed spikes are indicative of true seasonality or if they are merely outliers. A broader dataset will also help improve the robustness of our models, making them more adaptable and less prone to overfitting.

# Spatial Characteristics
In the global distribution map of climate-related topic web search, African countries enjoy the most popularity as a fraction of total searches in their region. Related search terms account for a moderate proportion in North America, Australia and South Asia, while it was not that popular in South America, Europe and other Asia regions. On a country level, Ethiopia and Zimbabwe had the largest proportion of climate search terms in their total searches.

Population can be a possible reason for the difference. In African countries, where few people have access to the Internet, the proportion of climate search terms may be magnified. In addition, international organizations and non-governmental organizations such as the United Nations Environment Program are headquartered in Africa, leading to related behaviors. The extent to which different countries are affected by climate change is also important. Countries like the Philippines, which are at the forefront of climate change, rank very high both in the interest index and in absolute values. People who live there are naturally more concerned about the climate.

```{r include=FALSE}
spatial <- read_excel("Data/spatial.xlsx")
```

```{r echo=FALSE}
world_map <- ne_countries(scale = "medium", returnclass = "sf")
merged_data <- left_join(world_map, spatial, by = c("name" = "Country"))
merged_data$Index <- as.numeric(as.character(merged_data$Index))
ggplot(data = merged_data) +
  geom_sf(aes(fill = Index)) +
  scale_fill_gradient(low = "white", high = 4, na.value = "lightgrey") +
  labs(title = "Global Climate Web Sesearch Map",
       fill = "Search Interest Index")

```


